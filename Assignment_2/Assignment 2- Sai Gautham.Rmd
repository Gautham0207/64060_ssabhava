---
title: "Assignment 2- Sai Gautham"
author: "Sai Gautham Sabhavathu"
date: "2/20/2022"
output: pdf_document
---

```{r setup}
#Question 1
#Installing all the packages required and importing the data by using the 
#read.csv function
library('caret')
library('ISLR')
library('dplyr')
library('class')

Bank <- read.csv("C:/Users/gauth/Downloads/UniversalBank.csv", sep = ',' )
 

#Keeping ID and ZIp as NULL as they are not required for the data.
Bank$ID <- NULL
Bank$ZIP.Code <- NULL
summary(Bank)

#Converting the Personal loan which is a categorical variable to a factor, 
#which classify a yes or no response.
Bank$Personal.Loan =  as.factor(Bank$Personal.Loan)

#Normalizing the data by making a normalization model first and then using 
#the min max method.

Model_normalized <- preProcess(Bank[, -8],method = c("center", "scale"))
Bank_normalized <- predict(Model_normalized,Bank)
summary(Bank_normalized)


#Partition data into testing and training sets
Train_index <- createDataPartition(Bank$Personal.Loan, p = 0.6, list = FALSE)
train.df = Bank_normalized[Train_index,]
validation.df = Bank_normalized[-Train_index,]

#Prediction 
To_Predict = data.frame(Age = 40, Experience = 10, Income = 84, Family = 2,
                        CCAvg = 2, Education = 1, Mortgage = 0, Securities.Account =
                          0, CD.Account = 0, Online = 1, CreditCard = 1)
print(To_Predict)
To_Predict_Normalized <- predict(Model_normalized,To_Predict)

Prediction <- knn(train= train.df[,1:7,9:12],
                  test = To_Predict_Normalized[,1:7,9:12],
                  cl= train.df$Personal.Loan,
                  k=1)
print(Prediction)



#Question 2 
#Finding the best value of K which balances between overfitting and underfitting.
set.seed(123)
Bankcontrol <- trainControl(method= "repeatedcv", number = 3, repeats = 2)
searchGrid = expand.grid(k=1:10)

knn.model = train(Personal.Loan~., data = train.df, method = 'knn', tuneGrid = searchGrid,trControl = Bankcontrol)

knn.model
#The best value of k is 3 which balances between the data overfitting and underfitting.


#Question 3
#The confusion matrix is shown below.
predictions <- predict(knn.model,validation.df)

confusionMatrix(predictions,validation.df$Personal.Loan)
#The matrix has a 95.1% accuracy.

#Question 4
#Classyifing the customer using the best K.
To_Predict_Normalized = data.frame(Age = 40, Experience = 10, Income = 84, Family = 2,
                                   CCAvg = 2, Education = 1, Mortgage = 0,
                                   Securities.Account =0, CD.Account = 0, Online = 1,
                                   CreditCard = 1)
To_Predict_Normalized = predict(Model_normalized, To_Predict)
predict(knn.model, To_Predict_Normalized)

#There is also a plot which clearly shoes the best value of K (3), which has the highest accuracy.
plot(knn.model, type = "b", xlab = "K-Value", ylab = "Accuracy")


#Question 5
#Dividing the data set into training, testing and validation sets.
train_size = 0.5
Train_index = createDataPartition(Bank$Personal.Loan, p = 0.5, list = FALSE)
train.df = Bank_normalized[Train_index,]


test_size = 0.2
Test_index = createDataPartition(Bank$Personal.Loan, p = 0.2, list = FALSE)
Test.df = Bank_normalized[Test_index,]


valid_size = 0.3
Validation_index = createDataPartition(Bank$Personal.Loan, p = 0.3, list = FALSE)
validation.df = Bank_normalized[Validation_index,]



Testknn <- knn(train = train.df[,-8], test = Test.df[,-8], cl = train.df[,8], k =3)
Validationknn <- knn(train = train.df[,-8], test = validation.df[,-8], cl = train.df[,8], k =3)
Trainknn <- knn(train = train.df[,-8], test = train.df[,-8], cl = train.df[,8], k =3)

confusionMatrix(Testknn, Test.df[,8])
confusionMatrix(Trainknn, train.df[,8])
confusionMatrix(Validationknn, validation.df[,8])

# From the above matrices, we determined the values of Test, Training and 
# Validation sets which are 96.3%,97.32% and 96.73% respectively. 
# It can be said that if the Training data has a higher accuracy than the other 
#sets , it would be called overfitting. Since, there is not much difference 
#between the Training, Test and validation set's accuracies, we can conclude
#that we have determined the best value of k

```


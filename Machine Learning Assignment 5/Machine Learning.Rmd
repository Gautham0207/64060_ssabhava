---
title: "Machine Learning Assignment 5"
author: "Sai Gautham Sabhavathu"
date: "4/17/2022"
output: pdf_document
---

```{r}
#Importing required libraries and packages
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
```



```{r}

#Importing dataset and creating data set with only numeric data
Cereals<- read.csv("C:/Users/gauth/Downloads/Cereals (1).csv")

Numeric_data <- data.frame(Cereals[,4:16])
```


```{r}
#Omitting missing values from the data
Numeric_data <- na.omit(Numeric_data)
```


```{r}
#Normalizing the data
Cereals_normalized <- scale(Numeric_data)
```



```{r}
#Applying hierarchical clustering to the data using Euclidean distance method to the normalized data.
Distance <- dist(Cereals_normalized, method = "euclidean")
Hierarchial_Clustering <- hclust(Distance, method = "complete")
```



```{r}
#Plotting of the dendogram.
plot(Hierarchial_Clustering, cex = 0.7, hang = -1)
```



```{r}
#Using Agnes function to perform clustering with single linkage, complete linkage
#, average linkage and Ward.
HierarchialClust_single <- agnes(Cereals_normalized, method = "single")
HierarchialClust_complete <- agnes(Cereals_normalized, method = "complete")
HierarchialClust_average <- agnes(Cereals_normalized, method = "average")
HierarchialClust_ward <- agnes(Cereals_normalized, method = "ward")
```



```{r}
#Determining the best method
print(HierarchialClust_single$ac)
print(HierarchialClust_complete$ac)
print(HierarchialClust_average$ac)
print(HierarchialClust_ward$ac)
```

#From the above, it is evident that the ward method is the best as it has the value of 0.9046042.


#Task 2-  Choosing the clusters:


```{r}
pltree(HierarchialClust_ward, cex = 0.7, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(HierarchialClust_ward, k = 5, border = 2:7)
SubGroup <- cutree(HierarchialClust_ward, k=5)

dataframe2 <- as.data.frame(cbind(Cereals_normalized,SubGroup))
```

#We will choose 5 clusters after observing the distance.

#Determining the structure and stability of the clusters. 



```{r}
#Creating Partitions
set.seed(123)
Partition_1 <- Numeric_data[1:50,]
Partition_2 <- Numeric_data[51:74,]
```



```{r}
#Performing Hierarchial Clustering,consedering k = 5.
AG_single <- agnes(scale(Partition_1), method = "single")
AG_complete <- agnes(scale(Partition_1), method = "complete")
AG_average <- agnes(scale(Partition_1), method = "average")
AG_ward <- agnes(scale(Partition_1), method = "ward")
cbind(single=AG_single$ac , complete=AG_complete$ac , average= AG_average$ac , ward= AG_ward$ac)
pltree(AG_ward, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(AG_ward, k = 5, border = 2:7)
cut_2 <- cutree(AG_ward, k = 5)
```



```{r}
#Calculating the centroids.
result <- as.data.frame(cbind(Partition_1, cut_2))
result[result$cut_2==1,]
centroid_1 <- colMeans(result[result$cut_2==1,])
result[result$cut_2==2,]
centroid_2 <- colMeans(result[result$cut_2==2,])
result[result$cut_2==3,]
centroid_3 <- colMeans(result[result$cut_2==3,])
result[result$cut_2==4,]
centroid_4 <- colMeans(result[result$cut_2==4,])
centroids <- rbind(centroid_1, centroid_2, centroid_3, centroid_4)
x2 <- as.data.frame(rbind(centroids[,-14], Partition_2))
```

```{r}

#Calculating the Distance.

Distance_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Distance_1)
dataframe1 <- data.frame(data=seq(1,nrow(Partition_2),1), Clusters = rep(0,nrow(Partition_2)))
for(i in 1:nrow(Partition_2)) 
  {dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1
cbind(dataframe2$SubGroup[51:74], dataframe1$Clusters)
table(dataframe2$SubGroup[51:74] == dataframe1$Clusters)
```
#From the above observation, we are getting 12 False and 12 True. Hence, we can conclude that the model is partially stable.

#3) The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?



```{r}
#Clustering Healthy Cereals.
Healthy_Cereals <- Cereals
Healthy_Cereals_na <- na.omit(Healthy_Cereals)
Clusthealthy <- cbind(Healthy_Cereals_na, SubGroup)
Clusthealthy[Clusthealthy$SubGroup==1,]
Clusthealthy[Clusthealthy$SubGroup==2,]
Clusthealthy[Clusthealthy$SubGroup==3,]
Clusthealthy[Clusthealthy$SubGroup==4,]
```



```{r}
#Mean ratings to determine the best cluster.
mean(Clusthealthy[Clusthealthy$SubGroup==1,"rating"])
mean(Clusthealthy[Clusthealthy$SubGroup==2,"rating"])
mean(Clusthealthy[Clusthealthy$SubGroup==3,"rating"])
mean(Clusthealthy[Clusthealthy$SubGroup==4,"rating"])
```


#From the above observations, the cluster 1 can choosen as it is the highest.